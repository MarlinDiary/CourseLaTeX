\documentclass[sigconf]{acmart}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{empty}

\usepackage{hyphenat}
\usepackage{url}
\def\UrlBreaks{\do\/\do-\do_\do.}

\begin{document}

\title{COMPSCI 778 Internship Journal - Milestone 2}

\author{Chenye Ni}
\email{cni586@aucklanduni.ac.nz}
\affiliation{%
  \institution{University of Auckland}
  \country{New Zealand}
}

\acmConference[COMPSCI 778]{}{December 2025}{Auckland, New Zealand}

\maketitle


\section{Work Environment}

This internship is part of the compulsory course COMPSCI~778 (Summer Internship) in the Master of Information Technology (MInfoTech) programme at the University of Auckland and has a duration of ten weeks. The internship is conducted in an on-campus mode, designed to provide students with a project development experience that closely resembles a real workplace, while still allowing them to receive guidance and support from supervisors in an academic environment. A dual-supervisor model is adopted: the academic supervisor is Vita~Tsai, who oversees academic progress and learning outcomes; the industry supervisor is Nick~Yager, who provides guidance and feedback from an industry perspective.

The internship workspace is located on Level~7 of the Fisher Building in central Auckland, at 18~Waterloo Quadrant, Auckland Central. This floor is dedicated to the COMPSCI~778 internship programme and accommodates around ten independent project teams. The workspace is equipped with basic working facilities, including external monitors and power outlets, enabling students to use their own laptops for development work. In addition, there is a shared common area with a microwave and refrigerator, which provides convenient options for meals and refreshments throughout the day.

The internship operates on a team-based structure --- each team consists of five MInfoTech students and is independently responsible for a different project. In my team, we adopted a flat management structure in which all five members held equal roles, with no fixed team leader, and decisions were made collaboratively with an emphasis on cooperation and shared responsibility.

During the internship period, the work schedule followed a full-time pattern, with five working days per week and eight working hours per day. The team adopted a daily stand-up meeting mechanism from agile development. At a fixed time each day, the team held an internal meeting. Each member reported in turn on three items: what they had completed the previous day, what they planned to work on that day, and whether they had encountered any blockers or needed support. In addition, a formal check-in meeting with the supervisors was scheduled every two weeks to review progress for the period, discuss any issues, and obtain guidance. In terms of collaboration tools, the team used Notion for project documentation and task tracking, GitHub for version control and collaborative development, and Microsoft Teams for day-to-day communication and online meetings.

Although this internship was conducted on campus --- technically within a university setting rather than an external company --- its working hours, team collaboration model, and project management processes closely simulated those of a real software development workplace. The flat team structure, daily stand-ups, and fortnightly check-ins created a valuable opportunity for me to practise agile development methodologies within a supportive academic environment.

\section{Project Description}

\subsection{Project Background}

Larder is a cross-platform intelligent recipe mobile app designed for everyday cooking, with support for both Android and iOS. We target users with fast-paced lifestyles who want to make food choices more efficiently within limited time. We chose this direction because recipe content today is highly fragmented: people repeatedly come across dishes that look delicious on websites, short-form videos, social media, or through friends and family, but these recipes are scattered across different platforms. This makes it difficult to save, organize, and reuse them in a unified way, and even harder to turn them into executable cooking plans and concrete shopping actions.

Through user research and everyday experience, we identified several recurring issues in existing tools across the cooking workflow. First, even after saving large numbers of recipes, users often struggle to quickly discover options that match their current tastes, time constraints, and available ingredients; the more they save, the harder it becomes to choose. Second, many recipes are presented in unstructured formats such as videos, making it difficult to follow steps reliably while cooking; repeatedly pausing, replaying, or scrubbing through a timeline adds significant interaction overhead. Meal planning and shopping decisions are also frequently disconnected: when planning meals for the next few days, users must juggle multiple constraints such as budget, health goals, dietary restrictions, variety, and avoiding repeats, and once a plan is set, they still face practical questions such as what to buy, how much to buy, and how to avoid duplicates given what is already in the fridge. Finally, during execution, wet or greasy hands make touch interaction inconvenient, yet users still want to move smoothly to the next instruction; this kind of hands-busy interaction is often overlooked despite being common in practice.

Based on the issues above, we designed the product around a coherent end-to-end user journey, forming a closed loop from discovery to planning, shopping, and cooking execution. For users who prefer structured planning, the app supports completing upcoming meal arrangements at a fixed time, adding desired dishes to a Meal Plan, and automatically generating a well-structured Grocery List. This helps users shop with a clear checklist and then cook day by day according to the plan. For users who plan less, they can first stock ingredients in the fridge and, right before cooking, scan the fridge to trigger recommendations and quickly receive actionable recipe candidates. Because we designed interaction with the assistant as a conversational entry point, users can also make ad hoc, personalized requests throughout the process, such as ``I want something lighter today but still high-protein,'' ``I only have 30 minutes,'' or ``I do not want to buy certain ingredients again.''

At the feature level, Larder centers on a Discover homepage that addresses the key need of efficiently finding what to cook from an overwhelming amount of content, using personalized recommendations based on user preferences and interaction signals. Around Discover, we provide recipe management, user accounts, and the combined capabilities of Meal Plan and Grocery, so that the flow from ``see a dish'' to ``decide to cook'' to ``schedule it for a day'' to ``generate a shopping list'' to ``start cooking'' can be completed within a single app. We also support importing recipes from external sources (for example, a URL, an image, or a video) so that scattered recipes can be brought under unified management.

To truly cover the execution stage, we emphasize a step-by-step guidance experience that can be operated without touching the screen. The Step-by-step page presents clear instructions and interaction controls, and provides timer assistance when a step includes time requirements. We also introduce non-contact interaction such as voice or gesture recognition, enabling users to move to the next step, repeat key instructions, or toggle timers smoothly when their hands are wet or greasy.

AI plays a role throughout the product, but we impose explicit constraints on its boundaries. To reduce hallucinations in recipe generation, the app is grounded in a real recipe database, which we clean, filter, and normalize into a searchable and reusable corpus; recommendations and retrieval are then performed using user preferences and contextual signals. For recipes generated by a large language model, we introduce a user feedback mechanism and use ratings to guide iterative improvements in content quality. We also make it explicit that we do not train our own models, and we do not pursue high-overhead, maintenance-heavy inventory management. The app does not provide medical-grade dietary advice, avoiding inappropriate claims in high-risk health contexts. Given practical constraints such as model invocation cost and token usage, controlling cost under limited context while maintaining answer quality is also treated as a key optimization goal.

From a systems perspective, the app adopts a cross-platform architecture to reduce multi-client development cost while maintaining consistent user experience. The mobile front end is built with React Native, the backend uses C\# and .NET to provide API services, and PostgreSQL serves as the core data store; this stack supports effective collaboration for multi-platform development within our team.


\subsection{Literature Review}

To build an intelligent cooking platform that integrates discovery, planning, and execution into a single coherent workflow, our implementation must be grounded in high-quality data and interaction-aware intelligence. Following the logic of our system pipeline, this review is organized around data engineering, decision support, and embodied interaction, and explains how prior work informs our design choices.

First, a core foundation of the project is how to process large-scale, largely unstructured recipe data. In their work on the Recipe1M+ dataset, Mar{\'{\i}}n et al.\ note that web-scale recipes are abundant but often suffer from inconsistent formatting, redundancy, and semantic noise, which can substantially degrade downstream retrieval and recommendation performance if used directly.\ \cite{marin2021recipe1mplus} This observation directly motivates our data engineering strategy: rather than plugging into raw data streams, we invest in deep cleaning and normalization of a large recipe database, converting it into a consistent, structured JSON representation. Building on cross-modal retrieval insights from Salvador et al., parsing loose recipe text into standardized fields (for example, explicit ingredient lists and quantified nutritional attributes) is a prerequisite for accurate matching between images or natural-language descriptions and recipes, as well as for efficient rendering and interaction on the front-end \textit{Discover} page.\ \cite{salvador2017learning_crossmodal}

For recommendation and search, we focus on the fact that real-world food decisions are often governed by complex constraints. Chen et al.\ formulate personalized food recommendation as constrained question answering, showing that similarity-based matching alone is frequently insufficient, and that systems should explicitly reason over constraints such as avoiding allergens or meeting a strict budget.\ \cite{chen2021personalized_food_kbqa} This perspective supports the design logic of our \textit{Discover} and query experience: beyond surfacing visually appealing recipe cards, the system must translate user preferences into executable filtering and constraint parameters, ensuring that recommended candidates remain personalized while strictly satisfying hard requirements.

To further improve interaction flexibility, we introduce a large language model (LLM) as an assistant, while explicitly mitigating domain-specific hallucination risks (for example, generating unsafe or implausible cooking instructions). Retrieval-augmented generation (RAG) is therefore a primary choice: Lewis et al.\ show that grounding generation in retrieved evidence from an external knowledge source can constrain model outputs to factual content.\ \cite{lewis2020rag} This leads to a clear technical route in our system: we do not train proprietary models, but instead retrieve verified recipe snippets from our curated corpus and inject them into the LLM context, improving reliability while also helping control token consumption. In addition, the ReAct framework proposed by Yao et al.\ inspires our design of the AI chat as an agent that can both reason and act, so the assistant can produce not only text responses but also structured actions such as ``add to grocery list'' when appropriate.\ \cite{yao2023react}

When users move from planning into the cooking execution stage (our step-by-step guidance), the interaction focus shifts to physical accessibility in a kitchen setting. Prior HCI work highlights that cooking is a context where attention is divided and hands are frequently wet or greasy, making touch interaction fragile and interruption-prone.\ \cite{nansen2014kitchen_kinesics} Be{\c{s}}evli et al.\ further report that non-contact gestural interaction can reduce both cognitive burden and hygiene-related concerns during cooking.\ \cite{besevli2022gestural_kitchen} These findings support our decision to integrate gesture control into the guidance interface. To implement low-latency hand tracking on mobile devices, Vakunov et al.\ demonstrate the feasibility of lightweight, monocular-camera-based, on-device hand tracking with MediaPipe Hands.\ \cite{vakunov2020mediapipe_hands} This evidence anchors our technical choice to run real-time gesture inference locally, helping close the common gap many recipe apps have at the final, in-kitchen execution stage.


\section{Cultivation and Application of Professionalism}

\subsection{Enhancement of Professional Ethics Awareness}

During the project development process, I gained a deeper understanding of the importance of professional ethics. In one of the daily stand-up meetings, the teaching assistant raised two key requirements concerning user data handling and AI-generated content: first, user data must be handled with extreme care, and sensitive information such as users' dietary preferences, health information, and location data must be protected with appropriate safeguards; second, there must be a robust feedback mechanism for AI-generated content that allows users to evaluate and correct recipes or other content recommended by the AI. I consider these requirements highly reasonable, and they also made me aware of the seriousness of these issues. As developers, we should not only focus on implementing features but also bear responsibility for our users, ensuring that the product meets ethical standards beyond its technical aspects. The cultivation of this sense of professional ethics is of profound significance for my future career development.

\subsection{Generation of Creative Ideas and Innovative Practice}

Many of the innovative features in the project stem from the collective intelligence of the team. Signature features such as gesture recognition, smart-scale connectivity, and refrigerator scanning did not arise from the inspiration of a single person but were gradually formed during team brainstorming sessions through communication and the collision of ideas among all members. This collaborative ideation process made me realize that good ideas often require the collision and iteration of different perspectives. An initial concept proposed by one person can often evolve into a more refined and feasible solution after being supplemented and challenged by other team members.

\subsection{Teamwork and Communication Skills}

Compared with other teams, the most distinctive characteristic of our team is the frequency of our meetings and the depth of our discussions. Because of our flat management structure, team members can express their views freely in a low-pressure and egalitarian environment. This model may lead to more disagreements than in other teams, but an obvious advantage is that we can freely point out the limitations of each other's viewpoints while always focusing on the issues themselves, without allowing discussions to escalate into emotional conflicts or involve personal feelings or concerns about saving face. Although frequent meetings appear to consume more time, in the end our solutions for each detail of the product were superior to any individual's initial proposal.

The benefits of the daily stand-up meetings for team collaboration are particularly evident. Through the stand-ups, we can clearly identify the work priorities for the day and plan the eight-hour work schedule in blocks of one to two hours, forming multiple actionable micro-tasks. The stand-ups also help us understand each member's progress, facilitating communication and mutual support. In this process, I learned a great deal from my teammates; even in the area of user experience design, where I had previously been most confident, I was able to identify my own shortcomings from their feedback, and this humble learning attitude benefited me greatly.

\subsection{Project Development Process}

The team adopted the Kanban methodology for project management and used GitHub Projects as the task-tracking tool. The task cards on the Kanban board clearly displayed three statuses---``To Do,'' ``In Progress,'' and ``Done''---allowing the entire team to see the project progress at a glance.

In terms of code management, the team strictly followed a branch-based development strategy. Each member carried out development work on their own feature branches and submitted pull requests upon completion. Each pull request had to be reviewed by at least two team members to confirm code quality, logical correctness, and adherence to coding standards before it could be merged into the main branch (\texttt{main}). This mechanism effectively ensured the stability of the codebase while also providing opportunities for team members to learn from one another.

The gesture recognition feature is a typical example of this process and originated from a brainstorming meeting. At that time, we were discussing the interaction design of the step-by-step cooking instruction page when one teammate suddenly asked, ``When the user is stir-frying and their hands are covered in oil, how do they go to the next step?'' This question stunned everyone for a moment---we had previously focused on the page layout and interaction design but had overlooked the basic scenario in which users' hands are not free while cooking. After discussion, the idea of gesture recognition ultimately prevailed: users would only need to wave their hands in front of the camera to turn the page, without touching the screen or relying on voice control that could be disturbed by noise from the range hood.

During implementation, the teammate responsible for this feature reported in the stand-up that they had encountered compatibility issues---the gesture recognition libraries they had tried behaved inconsistently on iOS and Android. After joint analysis, we decided to switch to a lower-level solution. Two days later, he demonstrated a smoothly running prototype at the stand-up; when he waved his hand and the screen turned pages accurately, the whole team applauded. In the code review stage, two other teammates identified shortcomings in handling edge cases, and after several rounds of revisions the feature was finally merged. From the initial question of ``What should users do when their hands are covered in oil?'' to the final usable feature, what truly made it possible was the collaboration of the entire team.

\section{Interim Outcomes and Reflection}

\subsection{Interim Outcomes}

After two weeks of development, the team has completed the requirements analysis and technology selection for the project and successfully implemented several core functional modules, including the home page interface, the diet-planning feature, the shopping-list feature, the basic AI dialogue feature, the step-by-step cooking instruction page, the user login and authentication system, and the gesture-based interaction feature. Overall, the project is progressing smoothly and has laid a solid foundation for subsequent feature development.

\subsection{Personal Reflection}

Before starting this internship, my greatest aspiration was to become an independent developer. In retrospect, however, this was less an aspiration and more a form of avoidance. Because I was not accustomed to handling interpersonal relationships, reluctant to interact with others, and unwilling to face potential friction in collaborative work settings, I set this goal for myself.

However, the collaborative experience of the past two weeks has completely changed my perspective. I have discovered that collaboration has a profound appeal of its own. I once mistakenly believed that I could do everything better than others, but this turned out not to be true. My teammates may not be as strong as I am in coding or engineering execution, but they have a much clearer understanding of task requirements, a better grasp of the instructor's expectations, and sharper thinking when it comes to defining features---all of which are areas where I can and should learn from them.

The internship, as it turns out, is not as intimidating as I had imagined. I have found that I can, in fact, get along well with others, and I have also noticed that I have changed significantly compared with the beginning of the internship. Most notably, when others present a different point of view, my first reaction is no longer displeasure; instead, I am genuinely interested in discussing their ideas further. This is because I have come to realize that I am not necessarily the smartest person in the room.

\subsection{Practical Application of Theoretical Knowledge}

In this internship, I applied knowledge from multiple courses to real-world development. The skills from COMPSCI~702 (Reverse Engineering) helped me extract key assets from other software---assets that would have been difficult for a five-person team to obtain through conventional means. The React knowledge I learned in COMPSCI~732 is closely related to React Native, enabling me to quickly ramp up on front-end development work. In addition, the group projects and software development methodologies covered in COMPSCI~718 and COMPSCI~719 were put into practice in our day-to-day team collaboration and project management.


\bibliographystyle{vancouver}
\bibliography{references}

\end{document}
