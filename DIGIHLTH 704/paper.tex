\documentclass[sigconf]{acmart}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\begin{document}

\title{Evaluating IDx-DR for Diabetic Retinopathy Screening}

\author{Chenye Ni}
\email{cni586@aucklanduni.ac.nz}
\affiliation{%
  \institution{University of Auckland}
  \country{New Zealand}
}

\acmConference[DIGIHLTH 704]{}{October 2025}{Auckland, New Zealand}

\maketitle

\section{Introduction and Scope}

Diabetic retinopathy (DR) is the leading cause of vision loss among people with diabetes worldwide. Recent epidemiology estimates that 22.27\% of individuals with diabetes have DR, with 103.12 million affected in 2020 and a projected 160.5 million by 2045 \citep{Teo2021Ophthalmology}. In the United States (US), the prevalence of DR in 2021 was 26.43\%, corresponding to 9.6 million people with DR, including 1.84 million with vision-threatening DR \citep{Lundeen2023JAMAOph}.

Despite strong evidence that early detection and timely treatment can prevent the vast majority of DR-related vision loss, current screening systems face substantial gaps. In 2017, only 54.1\% of Medicare Part B fee-for-service beneficiaries with diabetes received an annual eye exam, with marked disparities by race/ethnicity and insurance status \citep{Lundeen2019MMWR}. Analyses of 2005–2008 NHANES data indicate that 73\% of people with DR were unaware of their condition \citep{Gibson2012AJPM}. Shortages of eye-care specialists, maldistribution of services, and low care adherence further amplify this public health problem.

IDx-DR, now marketed as LumineticsCore, is an autonomous artificial intelligence diagnostic system developed by Digital Diagnostics Inc. In April 2018 it became the first fully autonomous AI diagnostic system authorized by the US Food and Drug Administration (FDA) via a De Novo request (DEN180001) \citep{FDA2018DEN180001}. The system uses convolutional neural networks to analyze non-mydriatic color fundus photographs acquired on the Topcon TRC-NW400; for each eye, one optic disc–centered and one macula-centered image are required \citep{FDA2018DEN180001}. At the point of care, it automatically detects more-than-mild diabetic retinopathy (mtmDR)—defined as Early Treatment Diabetic Retinopathy Study (ETDRS) severity level $\geq$35 and/or diabetic macular edema (DME)—and returns a binary clinical decision without specialist interpretation (mtmDR positive $\rightarrow$ refer; mtmDR negative $\rightarrow$ rescreen) \citep{Abramoff2018Pivotal,FDA2018DEN180001}.

This evaluation focuses on the system’s core function: detection of mtmDR. Under ETDRS grading, mtmDR corresponds to ETDRS level $\geq$35 or the presence of DME, encompassing moderate nonproliferative DR (NPDR) or worse and/or DME, including proliferative DR (PDR) \citep{ETDRS1991Report12}. The clinical rationale is clear: this threshold marks the inflection from lower to higher risk of progression and vision loss, warranting closer monitoring and potential intervention. Consistent with this, the American Diabetes Association’s 2025 Standards recommend prompt referral for any DME, moderate or worse NPDR, or any PDR \citep{ADA2025S12}. The intended users are non-ophthalmic clinicians in primary care settings, serving adult patients with diabetes without a prior DR diagnosis. Expected outcomes include increased early detection of mtmDR, timely referral, prevention of irreversible vision loss, and improved access and adherence by embedding screening within routine care pathways.

\section{Provenance and Knowledge Sources}

\subsection{Training data sources and composition}
The IDx\mbox{-}DR algorithm uses a deep learning architecture with multiple lesion\mbox{-}specific detectors for retinal biomarkers (e.g., microaneurysms, hemorrhages, exudates, neovascularization) \citep{Abramoff2018Pivotal}. Public sources do not disclose the provenance of the production training dataset for IDx\mbox{-}DR, including contributing institutions, geographic distribution, demographics, or sample sizes; this constitutes a transparency gap noted by ophthalmic AI reporting reviews \citep{Chen2024Transparency}. Prior technical descriptions from the developer group discuss patch\mbox{-}level training for lesion detectors, but the pivotal trial report and the FDA De Novo summary do not publish the production training set composition for IDx\mbox{-}DR \citep{Abramoff2018Pivotal,FDA2018DEN180001}. Ethical analyses argue that informed consent materials should include concise information about training data sources and characteristics \citep{Ursin2021InformedConsent}.

\subsection{Ethical oversight and validation dataset}
The pivotal prospective study enrolled 900 adults with diabetes across 10 U.S. primary care sites from January to July 2017, with Institutional Review Board approval at all sites and written informed consent from all participants; the study was registered on ClinicalTrials.gov (NCT02963441) \citep{Abramoff2018Pivotal}. The analyzable cohort had a median age of 59 years; 92.9\% had type 2 diabetes; race distribution was 63.4\% White and 28.6\% African American; the prevalence of mtmDR was 23.8\% \citep{Abramoff2018Pivotal}. The reference standard was established by the Wisconsin Fundus Photograph Reading Center (FPRC) using ETDRS grading with three validated graders and majority adjudication; diabetic macular edema was assessed with OCT per DRCR paradigms \citep{FDA2018DEN180001}.

\subsection{Model maintenance and update mechanism}
IDx\mbox{-}DR is a ``locked'' autonomous algorithm for the authorized indication. The system was locked before trial initiation and placed under an independent Algorithm Integrity Provider that escrowed code and outputs and interdicted sponsor access during the study \citep{Abramoff2018Pivotal}. The FDA De Novo decision describes a change\mbox{-}control protocol: any modification that could significantly affect clinical functionality or performance specifications triggers a new premarket submission (e.g., 510(k)) prior to commercial introduction \citep{FDA2018DEN180001}. This design favors safety and predictability but limits adaptation to population shifts, imaging advances, or emerging evidence without regulated updates.

\section{Usability and Interaction}

\subsection{System Interface and Clinical Workflow}

IDx\textsuperscript{\textregistered}\,–\,DR uses a three–tier architecture comprising a Windows client interface, a Topcon TRC\mbox{-}NW400 nonmydriatic fundus camera, and a cloud\mbox{-}based analysis server. The standard workflow requires operators to acquire four images per patient (two per eye: disc\mbox{-}centered and macula\mbox{-}centered), each with a minimum resolution of 1000\,$\times$\,1000 pixels. Images are submitted over a secure internet connection to the cloud service, which returns one of three binary outcomes within seconds to under one minute: “more\mbox{-}than\mbox{-}mild diabetic retinopathy detected,” “not detected,” or “insufficient image quality” \citep{Abramoff2018Pivotal,FDA2018DEN180001}.

\subsection{Nielsen Heuristic Evaluation}

\textbf{Heuristic 1: Visibility of system status (strong).} The client continuously surfaces processing state via progress indicators: "Exam received" $\rightarrow$ "Queued" $\rightarrow$ "Processing started" $\rightarrow$ "Processing complete," and a lower\mbox{-}right status reports internet "service available\slash unavailable." During acquisition, the client provides immediate quality feedback with specific problem labels (e.g., out\mbox{-}of\mbox{-}focus, field\mbox{-}of\mbox{-}view offset), enabling real\mbox{-}time correction by operators. An inter\mbox{-}operator reproducibility of 99.6\% supports consistency of status communication \citep{FDA2018DEN180001}.

\textbf{Heuristic 2: Match between system and the real world (mixed).} The interface employs terminology familiar to primary\mbox{-}care staff (macula, optic disc, ETDRS grading), yet technical constructs such as \textit{imageability} require additional explanation. The standardized 4\mbox{-}hour training includes a “retinal anatomical landmarks” module to address knowledge gaps. However, reliance on stable internet connectivity and cloud services contrasts with offline operation typical of some point\mbox{-}of\mbox{-}care devices, creating potential deployment barriers in settings with poor network infrastructure.

\textbf{Heuristic 3: User control and freedom (key weakness).} This is the most salient usability limitation. The workflow functions as a one\mbox{-}way decision gate: operators cannot override an algorithmic “insufficient image quality” determination. In an independent Dutch validation, IDx\mbox{-}DR rejected 33.7\% of images that human graders accepted at a rate of 80.4\%, highlighting limited operator override and a resubmission\mbox{-}only pathway without a human\mbox{-}quality override or expert consultation route \citep{vanDerHeijden2018Acta}.

\textbf{Heuristic 4: Consistency and standards (strong).} The UI follows standard Windows patterns (menu bar, dialogs, action buttons), offers complete multilingual UI and documentation translations, and presents a uniform configuration layout (checkboxes, dropdowns, “Save” button). Thumbnails, lists, and status views maintain visual consistency across tasks \citep{FDA2018DEN180001}.

\textbf{Heuristic 5: Error prevention (mixed).} Multiple safeguards are implemented: real\mbox{-}time quality feedback, automatic exam grouping to prevent image mix\mbox{-}ups, and a standardized four\mbox{-}image protocol. In the pivotal trial, a 4\% “insufficient image quality” rate reflects conservative thresholds that reduce false outputs, but it introduces operational friction—only 64.7\% completed the protocol on the first attempt; 8.5\% required one retry; 3.2\% two retries; 1.97\% three retries; 3.4\% four retries; and 0.5\% five retries. The system does not proactively warn prior to dilation; operators often infer small pupils only after repeated failures, and 23.6\% of participants ultimately required pharmacologic dilation \citep{Abramoff2018Pivotal}.

\textbf{Heuristic 7: Flexibility and efficiency of use (limited adaptability).} Novices and experts follow the same workflow. There are no documented keyboard shortcuts, batch operations, or expert accelerators. The only efficiency feature described is automatic grouping in the exam selection tool. This rigidity can conflict with primary\mbox{-}care throughput demands.

\textbf{Heuristic 10: Help and documentation (comprehensive resources).} IDx\mbox{-}DR provides an in\mbox{-}app electronic manual, an online training portal, email/phone support, and customer success representatives. A 4\mbox{-}hour training coupled with 10 hands\mbox{-}on practice cases enables previously untrained operators to achieve a 96\% success rate \citep{FDA2018DEN180001}.

\subsection{Accessibility Considerations}

With respect to accessibility, the system supports multiple languages but does not document accommodations for operators with disabilities: the interface relies on color coding (e.g., red boxes for quality issues) without a confirmed color\mbox{-}vision\mbox{-}safe alternative, and manual camera positioning requires physical dexterity without apparent assistive technology integration. Documentation demands a moderately high reading level; operators are expected to understand medical terminology (retinopathy, macula, ETDRS) and technical concepts (DICOM, protocol, imageability). A high\mbox{-}school or equivalent educational level is recommended \citep{FDA2018DEN180001}.

\section{Reverse Engineering or Explainability}

\subsection{Decision mechanism}

IDx\textsuperscript{\textregistered}\,–\,DR uses a two–stage pipeline that differs from end–to–end black–box classifiers. First, an image–quality algorithm validates basic acquisition constraints (field of view, focus, color balance, exposure). Only quality–eligible submissions proceed to diagnosis \citep{FDA2018DEN180001,Abramoff2018Pivotal}. 

For diagnosis, lesion–specific detectors analyze the four acquired images in parallel. Public sources describe convolutional models for disease detection but do not disclose the exact architecture or weights \citep{Abramoff2018Pivotal,FDA2018DEN180001}. Prior work and patents from the same group detail red–lesion (microaneurysm/hemorrhage) detection using morphology and matched filtering, which motivates the lesion–centric approach \citep{US7474775B2}. Clinically, microaneurysms are typically $\le$125\,\textmu m in diameter, whereas larger, irregular “red lesions” often reflect hemorrhages \citep{Shukla2023StatPearlsDR,Yu1998ArchOph}. 

A proprietary information–fusion step combines detector evidence across the four images and compares a scalar index against a fixed decision threshold to produce a binary result at the patient level. The participant–level decision uses a worst–eye rule: if either eye is positive for more–than–mild DR (mtmDR), the output is “mtmDR detected”; otherwise “mtmDR not detected” (or “insufficient image quality” if imageability fails) \citep{Abramoff2018Pivotal,FDA2018DEN180001}.

\subsection{Simplified illustrative logic}

\noindent\textit{This is an explanatory sketch for readability only; it is not the proprietary algorithm.}

\begin{verbatim}
IF input_has_4_images_per_protocol() THEN
  IF passes_quality(FOV, focus, color, exposure) THEN
    scores = {
      MA: detect_microaneurysms(imgs),
      HEM: detect_hemorrhage(imgs),
      EX: detect_exudates(imgs),
      NV: detect_neovascularization(imgs),
      DME: assess_center_involved_DME(imgs)
    }
    index = w1*MA + w2*HEM + w3*EX + w4*NV + w5*DME
    LEFT  = eye_index(scores, "left")
    RIGHT = eye_index(scores, "right")
    PATIENT = max(LEFT, RIGHT)   // worst-eye logic
    IF PATIENT >= THRESHOLD THEN
      OUTPUT = "mtmDR detected"      
    ELSE
      OUTPUT = "mtmDR not detected"  
    ENDIF
  ELSE
    OUTPUT = "insufficient image quality"  
  ENDIF
ELSE
  OUTPUT = "protocol incomplete"           
ENDIF
\end{verbatim}

\subsection{Rule pattern examples for clinical intuition}

\noindent\textit{For intuition only, not device rules.} Features that often drive “mtmDR+” under ETDRS–based reference standards include: widespread intraretinal hemorrhages, venous beading, or IRMA consistent with moderate–to–severe NPDR; any neovascularization; or center–involved DME. Conversely, absence of these findings with few microaneurysms and no DME aligns with “mtmDR–” \citep{ETDRS1991Report12,ADA2025S12}.

\subsection{Transparency and explainability}

External transparency is limited. The authorized device returns only a discrete decision at point of care: “mtmDR detected,” “mtmDR not detected,” or “insufficient image quality.” Public labeling and the pivotal report do not provide saliency maps, per–lesion overlays, confidence scores, per–eye probabilities, or severity grades for clinical users \citep{FDA2018DEN180001,Abramoff2018Pivotal}. Bioethics analyses argue that informed consent materials should disclose high–level training–data characteristics and decision logic to support patient autonomy \citep{Ursin2021InformedConsent}.

\section{Clinical Quality and Impact}

\subsection{Diagnostic accuracy}

In the pivotal primary care trial, IDx\textsuperscript{\textregistered}\,–\,DR achieved sensitivity 87.2\% (95\% CI: 81.8–91.2) and specificity 90.7\% (95\% CI: 88.3–92.7), correctly identifying 173 of 198 mtmDR\,+ cases among 819 analyzable participants \citep{Abramoff2018Pivotal}. The device detected all ETDRS level~43 or higher cases, including all 16 proliferative DR cases; sensitivity for vision‐threatening DR was 97.4\% (37/38). For macular edema, sensitivity was 96.6\% for clinically significant DME from fundus photos and 84.2\% for center‐involved DME by OCT \citep{Abramoff2018Pivotal,FDA2018DEN180001}.

A 2025 meta‐analysis of 13 studies (13{,}233 participants) reported pooled sensitivity 0.95 (95\% CI: 0.82–0.99), pooled specificity 0.91 (95\% CI: 0.84–0.95), and SROC AUC 0.95 for IDX‐DR, with performance varying across settings \citep{Khan2025AJO}. Real‐world studies illustrate this variability: in the Hoorn primary care system the sensitivity was 68\% (95\% CI: 56–79\%) and specificity 86\% (95\% CI: 84–88\%) \citep{vanderHeijden2018Hoorn}. In a Polish screening cohort directly comparing two systems, IDx‐DR showed sensitivity 99.3\% and specificity 68.9\% for referable DR, with PPV 48.1\% and NPV 99.5\%; RetCAD showed sensitivity 89.4\% and specificity 94.8\% \citep{Grzybowski2025OphTher}.

\subsection{Alignment with clinical guidelines}

IDx\textsuperscript{\textregistered}\,–\,DR is authorized to detect “more than mild DR,” operationalized as ETDRS level~$\geq$35 and/or DME, using a worst‐eye patient‐level decision \citep{FDA2018DEN180001,Abramoff2018Pivotal}. This threshold aligns with guidance that patients with moderate or worse NPDR and/or DME warrant prompt specialist evaluation, while those with no or mild DR may be followed at longer intervals \citep{ADA2025S12}. The conservative mtmDR cutoff prioritizes sensitivity to minimize missed disease, which can increase referral volume relative to stricter treatment thresholds; deployment should match local capacity and follow structured referral pathways \citep{ADA2025S12,FDA2018DEN180001}.

\section{Summary and Reflection}

As the first fully autonomous AI diagnostic system authorized by the FDA, IDx\textsuperscript{\textregistered}\,–\,DR fills a crucial gap in diabetic retinopathy screening. Its core value lies in deploying specialist\mbox{-}level diagnostic capability to the front lines of primary care, directly addressing foundational pain points in traditional screening pathways. Evidence from the ACCESS randomized controlled trial is most compelling: the AI arm achieved a 100\% screening completion rate (95\% CI: 95.5–100\%), whereas the standard referral arm reached only 22\% (95\% CI: 14.2–32.4\%), a difference of 78 percentage points (\(p < 0.001\)); more importantly, the system fully eliminated baseline disparities in screening attributable to race, income, and educational attainment \citep{Wolf2024NatCommun}.

However, the very autonomy that constitutes its core advantage also raises profound ethical challenges. The opacity of the algorithm conflicts with the verifiability principles of evidence\mbox{-}based medicine—clinicians cannot understand the AI’s reasoning process, cannot explain the diagnostic basis to patients, and cannot effectively override when skeptical. When clinicians passively accept algorithmic determinations, independent analytic capacity and clinical judgment may atrophy over time. The principle of informed consent is likewise challenged—patients cannot engage the algorithm in dialogue, cannot learn what features the AI “saw,” and cannot grasp the degree of diagnostic certainty \citep{Ursin2021InformedConsent}. To its credit, Digital Diagnostics has explicitly accepted legal responsibility for system performance, setting an important precedent for accountability frameworks in autonomous AI.

The system’s technical limitations must also be acknowledged. A 10–32\% false\mbox{-}positive rate drives over\mbox{-}referral; device dependence (compatibility limited to Topcon cameras), the constraints of a locked algorithm without continual learning, and restrictive exclusion criteria all temper clinical generalizability and fitness for purpose.

Based on the foregoing analysis, this evaluation adopts a conditionally favorable recommendation. The system is well suited for deployment in primary care and diabetes specialty clinics—especially in underserved areas with ophthalmologist shortages—best used as a complementary tool within a comprehensive eye\mbox{-}health strategy rather than a replacement. Key directions for improvement include integrating explainability techniques, reducing the false\mbox{-}positive rate, expanding device compatibility, and addressing the digital divide through policy measures—ensuring adequate Medicaid reimbursement, providing implementation grants for safety\mbox{-}net institutions, and supporting rural broadband infrastructure.

The broader significance of IDx\textsuperscript{\textregistered}\,–\,DR lies in demonstrating the technical feasibility of autonomous medical AI, but its success turns on a fundamental choice: whether to deploy primarily in affluent systems where implementation is easiest, or to prioritize the marginalized populations with the greatest unmet need. Only when innovation proactively bridges rather than widens health inequities can AI credibly fulfill its promise of improving care for all.

\bibliographystyle{vancouver}
\bibliography{references}

\end{document}
