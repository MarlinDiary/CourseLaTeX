\documentclass[sigconf]{acmart}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{empty}

\usepackage{hyphenat}
\usepackage{url}
\def\UrlBreaks{\do\/\do-\do_\do.}

\begin{document}

\title{Smartphone On-device AI for DR Pre-screening in Primary Care}

\author{Chenye Ni}
\email{cni586@aucklanduni.ac.nz}
\affiliation{%
  \institution{University of Auckland}
  \country{New Zealand}
}

\acmConference[DIGIHLTH 704]{}{October 2025}{Auckland, New Zealand}

\maketitle

\section{Introduction}

Diabetic retinopathy (DR) is the most common blinding microvascular complication of diabetes. As global diabetes prevalence continues to rise, screening and early\hyp{}intervention systems are under substantial pressure \cite{IDFAtlas2025}. In 2020, DR accounted for about 1.07 million cases of blindness and a further 3.28 million cases of moderate\hyp{}to\hyp{}severe vision impairment worldwide---outcomes that are largely preventable with timely screening \cite{VLEG_Eye_2024}.

In Aotearoa New Zealand, DR is a leading cause of vision loss among working\hyp{}age adults \cite{RANZCO_2022_DR_Screening_NZ}. Yet meaningful coverage gaps persist, disproportionately affecting M\={a}ori and Pasifika communities. Evidence from Te Tai Tokerau (Northland) and the wider Auckland region highlights multiple barriers---including transport constraints, time costs, limited disease awareness, and rigid booking systems---that collectively depress screening participation in high\hyp{}risk groups, delay referrals that could be avoided, and exacerbate inequities \cite{Wolpert2023_NZMJ}.

Artificial intelligence offers a path to narrow these gaps. Deep\hyp{}learning systems for DR detection have achieved ophthalmologist\hyp{}level performance on fundus photographs \cite{Gulshan2016JAMA}. Moreover, the successful deployment of autonomous systems such as IDx\hyp{}DR in primary\hyp{}care settings demonstrates that automated AI screening can operate safely and effectively outside specialist clinics, moving the screening ``front door'' closer to where patients receive routine care \cite{Abramoff2018NPJDM,FDA_DEN180001_2018}.

However, current AI screening solutions still face deployment constraints. IDx\hyp{}DR, for example, depends on a tabletop non\hyp{}mydriatic fundus camera, presupposing a fixed site, stable power, and standardized operating conditions \cite{DigitalDiagnostics_Indications_2025}. Such infrastructure requirements limit flexibility in remote communities, mobile clinics, and settings without dedicated rooms---challenges that are particularly salient for rural M\={a}ori and Pasifika populations.

This report proposes an AI\hyp{}enabled mobile DR pre\hyp{}screening prototype tailored for non\hyp{}ophthalmic personnel in New Zealand primary healthcare. Target users include community nurses, general practitioners, and M\={a}ori health service workers, operating in community clinics, mobile units, and home\hyp{}visit contexts. By combining portable smartphone fundus imaging with automated AI analysis, the prototype aims to extend screening reach to the hardest\hyp{}to\hyp{}access populations, substantially increase coverage while maintaining clinical accuracy, and ultimately reduce avoidable vision loss and mitigate health inequities.

\section{Background and Literature Review}

\textbf{Technical feasibility.} The combined use of smartphone\hyp{}based fundus imaging and AI has been empirically validated for accuracy across clinical and community settings. Rajalakshmi \textit{et~al.} (2018) reported 95.8\% sensitivity for automated DR detection on smartphone photographs, demonstrating that robust signal can be extracted from handheld capture \cite{Rajalakshmi2018Eye}. Extending this evidence, Malerbi \textit{et~al.} (2024) showed that a handheld, single\hyp{}image protocol achieved 90.48\% sensitivity and 90.65\% specificity, indicating that simplified capture workflows can still support reliable grading \cite{Malerbi2024OphthalmolSci}. To meet deployment needs in remote or connectivity\hyp{}limited settings, an on\hyp{}device approach is essential. Community\hyp{}based studies have verified the feasibility of \emph{offline} AI systems \cite{Natarajan2019JAMAOphthalmol,Jain2021IJO}, showing that DR algorithms can execute fully on device without network access, thereby enabling immediate feedback loops, reducing privacy risks, and maintaining service continuity during outages.

\textbf{Operational feasibility.} Success hinges on the intended users---non\hyp{}ophthalmic personnel working in primary care. Evidence indicates that operational barriers can be mitigated through brief, structured training and continued remote feedback. In an urban primary\hyp{}care study, Queiroz \textit{et~al.} (2020) evaluated inexperienced health workers using a handheld smartphone\hyp{}based camera; following short training and ongoing tele\hyp{}supervision, by day~7 the proportion of images adequate for clinical decision\hyp{}making stabilized above 80\% \cite{Queiroz2020ActaDiabetol}. Complementarily, a nursing study reported 75\% agreement between nurse\hyp{}performed screening and ophthalmologist assessment, consistent with moderate agreement and supporting safe task\hyp{}shifting under protocolized supervision \cite{FernandezGutierrez2023NursRep}.

\textbf{Standards and local frameworks.} To ensure rigor, usability, and local appropriateness, the prototype aligns with international standards and Aotearoa\hyp{}specific guidance. The design process will follow ISO~9241\hyp{}210 human\hyp{}centred design principles \cite{ISO9241-2102019}, requiring iterative cycles driven by explicit understanding of users, tasks, and contexts of use, with formative usability evaluations to optimize flows and error handling. Given the centrality of AI, reporting and design decisions will be aligned with the CONSORT\hyp{}AI extension \cite{Liu2020CONSORTAI}; this includes pre\hyp{}specifying human--AI interaction boundaries, eligibility criteria for input data, handling rules for ungradable images, and an error analysis plan. Finally, to address equity gaps experienced by M\={a}ori and Pasifika communities, the work will adopt an Aotearoa co\hyp{}design integrity framework targeting Level~3 shared governance and decision\hyp{}making, in order to build trust and ensure cultural appropriateness throughout requirements, prototyping, and evaluation \cite{GoodwinBoulton2024ShortGuide}.

\section{Domain Model}

Figure~\ref{fig:domain} presents the core concept structure and workflow of the AI\hyp{}enabled mobile DR pre\hyp{}screening application. The model centers on two stakeholder groups: primary users (community nurses, general practitioners, or M\={a}ori health workers) and patients.

\textbf{Data inputs.} The workflow begins with two inputs. (1) \emph{Patient registration:} the user records the patient National Health Index (NHI), basic history, and obtains informed consent. (2) \emph{Image acquisition:} the user captures bilateral fundus images using a portable smartphone\hyp{}based camera.

\textbf{AI processing pipeline.} First, images undergo pre\hyp{}processing to optimize quality. Next is a decision point: \emph{image\hyp{}quality assessment}. An on\hyp{}device classifier evaluates analyzability; if inadequate, the system prompts immediate re\hyp{}capture, forming a just\hyp{}in\hyp{}time feedback loop. If adequate, the image proceeds to \emph{DR classification}, where a deep convolutional network assigns a five\hyp{}level ordinal severity (including an indicator for more\hyp{}than\hyp{}mild DR, mtmDR) and outputs a confidence score.

\textbf{Structured outputs.} The system generates an immediate screening report that displays the DR risk grade, a clear referral recommendation, and an image\hyp{}quality score.

\textbf{Clinical pathway.} The primary user reviews the report, makes a decision based on the recommendation (for example, referral for high\hyp{}risk patients), and communicates the result to the patient, closing the screening loop.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{domain.png}
  \caption{Domain model of the AI\hyp{}enabled mobile DR pre\hyp{}screening application.}
  \label{fig:domain}
\end{figure}

\section{System Design and Architecture}

We design the architecture as a fully on\hyp{}device mobile application that minimizes reliance on network connectivity, thereby improving accessibility in remote settings and protecting patient privacy. Figure~\ref{fig:system} details this multilayer architecture, covering the complete flow from the user interface to local data persistence.

\subsection{Frontend interaction and validation layer}

The system flow begins at the frontend UI, which consists of two core screens:

\emph{Registration screen.} This is the primary entry point for users (e.g., community nurses) to record patient information. It includes fields for the National Health Index (NHI), basic demographics, and informed consent.

\emph{Camera screen.} This is the central interaction surface. It provides a real\hyp{}time viewfinder to assist alignment on the fundus and includes controls to capture bilateral retinal images.

When a user submits data or images, the input\hyp{}validation module starts immediately. This module performs local checks, such as verifying the NHI format and the completeness of image metadata. To ensure data accuracy, when a network is available it asynchronously queries an external NHI service to validate identity; however, the core screening flow continues to function offline.

\subsection{Core AI processing pipeline}

After validation, the workflow controller takes over. It coordinates all steps in the AI pipeline in sequence.

\textbf{1. Image preprocessing.} All input images are first routed to the preprocessor. This module performs key standardization steps: resizing to $512\times512$ pixels, normalizing brightness, and enhancing contrast. These steps are essential to feed subsequent AI models with consistent, quality\hyp{}optimized inputs.

\textbf{2. Quality\hyp{}assessment model.} We select MobileNetV3\nobreakdash-Small as the assessment model. MobileNetV3 is designed for mobile CPU optimization and low\hyp{}resource scenarios, which is critical for on\hyp{}device computer vision and an immediate feedback loop \cite{Howard2019MobileNetV3}. The model outputs a quality score in $[0,1]$. If the score is below the $0.7$ threshold, the workflow controller rejects the image and guides the user to retake it.

\textbf{3. DR classification model.} This module adopts EfficientNet\hyp{}B0. The model delivers strong performance on five\hyp{}level DR grading and is known for its balance between computational efficiency and accuracy \cite{Arora2024SciRepEfficientNetDR}.

\subsection{Report generation and data persistence}

After the AI pipeline completes, the report generator compiles patient information from the validation module, the quality score from the assessment model, and the DR grade with confidence from the classification model. It then produces a structured PDF screening report and automatically assigns referral urgency based on the DR grade.

The generated report appears immediately on the results dashboard. Users can view risk indicators, browse the history of screenings, and, when needed, submit high\hyp{}risk cases via an external e\nobreakdash-Referral interface.

To enable offline functionality and preserve privacy, all data are stored securely on the device. An encrypted SQLite database stores patient records and screening history. Captured images are temporarily stored as encrypted local files. The two core AI model files are also stored locally and loaded by the AI module at app launch, ensuring that the core screening functionality does not require internet connectivity.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{system.png}
  \caption{System design and multilayer architecture of the mobile DR pre\hyp{}screening application.}
  \label{fig:system}
\end{figure}

\section{Prototype and Mock\hyp{}ups}

Figure~\ref{fig:ux} presents the core user experience (UX) flow and explains how the design meets the specific needs of non\hyp{}ophthalmic users.

\textbf{Patient Registration.} The workflow begins with a clear form for capturing the National Health Index (NHI) number and basic information. A mandatory \texttt{Informed Consent} toggle acts as a critical ethical control point; until consent is obtained, the \texttt{Start Capture} button remains disabled.

\textbf{Image Capture.} This is the primary interaction screen for non\hyp{}specialists. It is more than a camera viewfinder; it integrates an AI real\hyp{}time guidance system. A circular on\hyp{}screen prompt box and a top status indicator provide immediate, proactive feedback before capture, helping users achieve proper focus and alignment to maximize first\hyp{}try success.

\textbf{Image Rejection.} This is the most important error\hyp{}handling example in the prototype. If a captured image is poor quality, the system does not display a generic failure. Instead, it provides clear, actionable AI feedback so that, after tapping \texttt{Retake Image} to return to \emph{Image Capture}, the user can more easily correct the previous error.

\textbf{Processing.} This page is the transition on the success path once image quality has passed evaluation. It provides positive confirmation that the difficult capture task has been completed and shows a loading animation to make explicit that a second\hyp{}stage AI analysis is running on device.

\textbf{Results.} This page is the AI deliverable. The most prominent elements are the top risk indicator and explicit action instructions, ensuring that the first thing users see is what to do rather than complex clinical data. Detailed AI outputs and confidence scores are intentionally placed in a collapsible \texttt{AI Analysis Details} region, applying progressive disclosure to avoid information overload without sacrificing explainability.

\textbf{Confirmation.} This page closes the loop after the user taps \texttt{Generate e\nobreakdash-Referral}. It acts as a receipt, clearly confirming two completed tasks: ``e\hyp{}Referral has been generated'' and ``Report saved to local device storage''. The \texttt{Screen New Patient} button provides the next clear starting point.


\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{ux.png}
  \caption{Prototype UX flow and key screens of the mobile DR pre\hyp{}screening application.}
  \label{fig:ux}
\end{figure}

\section{Ethical and Practical Considerations}

\subsection{Algorithmic fairness and bias}
Although AI performs well for DR detection, algorithmic bias may exacerbate existing health inequities. Performance of deep\hyp{}learning systems depends strongly on the composition of training datasets. If the core classifier is trained mainly on European or Asian fundus images, diagnostic accuracy may decline for M\={a}ori and Pasifika populations. A study in Indigenous Australians confirms that independent external validation is necessary before applying AI to minority groups \cite{Chia2024BJO_IndigenousDR}. Therefore, before entering clinical trials, the model must be re\hyp{}validated on a dataset that includes sufficient M\={a}ori and Pasifika patients, in order to quantify and correct any potential subgroup performance differences.

\subsection{Data sovereignty and privacy}
To truly serve M\={a}ori communities, the project must respect and enact the principles of M\={a}ori data sovereignty \cite{TMR2018Principles}. Our on\hyp{}device processing and encrypted local\hyp{}storage architecture directly responds to these principles. This design reduces the risk of data leakage at a technical level and, more importantly, aligns procedurally with expectations of collective and individual control over information, forming a cornerstone for trust with communities.

\subsection{Consent, explainability, and clinical integration}
The \texttt{Informed Consent} toggle ensures that consent is obtained before operation and that patients understand AI can err. Showing AI confidence scores on the results page is key to practicing explainable AI; it supports nurses' clinical judgment and helps them explain findings to patients. To mitigate automation bias (over\hyp{}reliance on AI), pre\hyp{}deployment training must establish clear guidance: when AI outputs conflict with clinical judgment, human judgment takes precedence.

\subsection{Validation, deployment, and monitoring}
A responsible AI tool requires a complete pathway from validation to continuous monitoring. Prior to deployment, and following the CONSORT\hyp{}AI extension \cite{Liu2020CONSORTAI}, a prospective clinical trial should be conducted in real New Zealand primary\hyp{}care settings to assess impacts on screening coverage and clinical outcomes. After deployment, a continuous post\hyp{}deployment performance\hyp{}monitoring mechanism is required, with regular audits to guard against model drift. Monitoring must include ongoing analysis of AI error cases across ethnic groups to ensure sustained fairness.

\section{Limitations and Future Work}

\textbf{Hardware and environment heterogeneity.} The core technical challenge is uncertainty from device and environmental heterogeneity. Although smartphone\hyp{}based fundus imaging is feasible, camera sensor specifications, lens optics, and processing pipelines vary widely across devices. In remote clinics, these hardware factors interact with uncontrolled ambient lighting, handheld stability, and patient cooperation, producing complex performance variability.

\textbf{Clinical accountability and governance.} A second constraint is ambiguity in clinical responsibility. When non\hyp{}ophthalmic nurses make referral decisions based on AI outputs, later misclassification raises unresolved questions within current legal and governance frameworks: developer liability, deploying institution responsibility, or operator accountability. This uncertainty creates legal risk and can erode patient trust, becoming an implicit barrier to clinical adoption.

\textbf{System capacity constraints.} The prototype assumes adequate downstream ophthalmology capacity. If screening coverage increases while referral pathways are saturated, a diagnostic bottleneck shift may occur: high\hyp{}risk patients are identified promptly but experience treatment delays due to limited specialist resources, increasing anxiety and inequity.

\textbf{Future work.} Priority tasks include prospective studies in New Zealand primary\hyp{}care settings to quantify diagnostic performance across device\hyp{}environment combinations and qualitative interviews to explore user experience. In parallel, collaboration with legal experts and health administrators is required to establish a clear governance framework and shared\hyp{}responsibility model, creating a foundation for safe clinical integration.





\bibliographystyle{vancouver}
\bibliography{references}

\end{document}
